{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slike/linreg_1.jpg\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "# Nadzorovano učenje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "\n",
    "Scenarij pri metodah *nadzorovanega modeliranja* je pogosto naslednji. Podatki so predstavljeni s pari\n",
    "\n",
    "$$ {(\\vec{x}_1, y_1), (\\vec{x}_2, y_2), ... (\\vec{x}_n, y_n)} $$\n",
    "\n",
    "kjer $\\vec{x}_i$ imenujemo *neodvisne*, $y_i$ pa *odvisne* spremenljivke.  Zanima nas *preslikava* $h(\\vec{x})$, ki vrednosti neodvisne spremenljivke slika v odvisne, z napako $\\epsilon_i$. Torej,\n",
    "\n",
    "$$ y_i = h(\\vec{x_i}) + \\epsilon_i $$ \n",
    "\n",
    "Spremenljivke $\\vec{x}_i$, $y$ so v splošnem lahko zvezne, diskretne in druge. Preslikava $h(\\vec{x})$ predstavlja *model* podatkov. Preslikava je lahko poljubna matematična funkcija (ali tudi algoritem, program), ki je odvisna od enega ali več *parametrov*. \n",
    "\n",
    "Strojno učenje pogosto pojmujemo kot iskanje parametrov (ali kar funkcije same) tako, da bo napaka $\\epsilon_i$ karseda majhna. \n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "The scenario for *controlled modeling* methods is often the following. The data is presented with couples\n",
    "\n",
    "$$ {(\\vec{x}_1, y_1), (\\vec{x}_2, y_2), ... (\\vec{x}_n, y_n)} $$\n",
    "\n",
    "where $\\vec{x}_i$ is called *independent*, and $y_i$ *dependent* variables. We are interested in the *mapping* $h(\\vec{x})$, which maps the values of the independent variable to the dependent, with the error $\\epsilon_i$. So,\n",
    "\n",
    "$$ y_i = h(\\vec{x_i}) + \\epsilon_i $$\n",
    "\n",
    "The variables $\\vec{x}_i$, $y$ can generally be continuous, discrete and others. The $h(\\vec{x})$ mapping represents the *model* of the data. The mapping can be any arbitrary mathematical function (or also an algorithm, a program) that depends on one or more *parameters*.\n",
    "\n",
    "Machine learning is often regarded as a search for parameters (or of the function itself) so that the error $\\epsilon_i$ is as small as possible.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slike/linreg_2.png\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "## Linearna regresija\n",
    "\n",
    "Linearna regresija je primer enostavnega modela, kjer predpostavljamo:\n",
    "* tako odvisne kot neodvisne spremenljivke so realna števila\n",
    "* odvisna spremljivka je linearna kombinacija neodvisnih \n",
    "* napaka $\\epsilon$ je normalno porazdeljena z upanjem $\\mu_{\\epsilon}=0$ in neznano varianco\n",
    "\n",
    "Odvisne spremenljivke so v splošnem vektorji v $p$-dimenzionalnem prostoru realnih števil, $\\vec{x} = (x_1, x_2, ... x_p)$.\n",
    "\n",
    "**Model** je oblike\n",
    "\n",
    "$$ h(\\vec{x}) = \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\beta_0$$\n",
    "\n",
    "kjer vektor $\\vec{\\beta} = (\\beta_0, \\beta_1, ... \\beta_p)$ predstavlja neznane parametre oz. koeficiente. Model je torej premica (pri $p=1$) oz. ravnina v $p$-dimenzionalnem prostoru. \n",
    "\n",
    "\n",
    "Učenje predstavlja iskanje (optimizacijo) parametrov $\\vec{\\beta}$ s ciljem zmanjšanja povprečne napake v podatkih.\n",
    "\n",
    "$$ \\text{min}_{\\beta} \\frac{1}{n} \\sum_1^{n} (y_i - h(\\vec{x}_i))^2 = \\frac{1}{n} \\sum_1^{n} \\epsilon^2 $$\n",
    "\n",
    "Vrednost zgornjega izraza se imenuje **srednja kvadratična napaka** (ang *mean squared error* ali MSE). Iz statističnega vidika pa predstavlja **nepojasnjeno varianco**.\n",
    "\n",
    "Algoritmov za minimizacijo zgornjega izraza tokrat ne bomo izpeljevali, temveč se raje osredotočimo na praktično uporabo. Več napotkov je na voljo <a href=\"http://www.stat.cmu.edu/~hseltman/309/Book/chapter9.pdf\">tukaj</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Linear regression\n",
    "\n",
    "Linear regression is an example of a simple model where we assume:\n",
    "* both dependent as independent variables are real numbers\n",
    "* the dependent variable is a linear combination of independent ones\n",
    "* The $\\epsilon$ error is normally distributed with the hope of $\\mu_{\\epsilon}=0$ and unknown variance\n",
    "\n",
    "The dependent variables are in general vectors in the $p$-dimensional space of real numbers, $\\vec{x} = (x_1, x_2, ... x_p)$.\n",
    "\n",
    "**The model** is of form\n",
    "\n",
    "$$ h(\\vec{x}) = \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\beta_0$$\n",
    "\n",
    "where the vector $\\vec{\\beta} = (\\beta_0, \\beta_1, ... \\beta_p)$ represents unknown parameters or coefficients. The model is therefore a line (for $p=1$) or a plane in $p$-dimensional space.\n",
    "\n",
    "Learning is the search (optimization) of parameters $\\vec{\\beta}$ with the aim of reducing the average error in the data.\n",
    "\n",
    "$$ \\text{min}_{\\beta} \\frac{1}{n} \\sum_1^{n} (y_i - h(\\vec{x}_i))^2 = \\frac{1}{n} \\sum_1^{n} \\epsilon^2 $$\n",
    "\n",
    "The value of the above term is called **the mean square error** (or MSE). From a statistical point of view it represents the **unexplained variance**.\n",
    "\n",
    "At this time, we will not derive algorithms for minimizing the above expression, but rather focus on practical use. More information is available <a href=\"http://www.stat.cmu.edu/~hseltman/309/Book/chapter9.pdf\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'jpg'\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('PR.mplstyle')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Začnimo s preprostim primerom z eno neodvisno spremenljivko $x$ ter odvisno spremenljivko $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Let's start with a simple example with one independent variable $x$ and a dependent variable $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"podatki/sintetični/data_A.txt\")\n",
    "x    = data[:, [0]]\n",
    "y    = -data[:, [1]]\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y, \"k.\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "lang": "sl"
   },
   "source": [
    "Podatki kar dobro spominjajo na premico. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "The data strongly resembles a line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "lang": "sl"
   },
   "source": [
    "Poizkusimo poiskati linearni model, ki bo zmanjšal srednjo kvadratično napako.\n",
    "\n",
    "Na levi sliki prikazujemo <font color=\"green\">vrednosti modela</font> za vse vrednosti $x$ na danem intervalu.\n",
    "\n",
    "Desna slika pa prikazuje vrednost <i>ostankov</i> $y_i - h(\\vec{x}_i)$ .  Bolje, kot se model prilega podatkom, manj povezana bosta odvisna spremenljivka in ostanek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Let's try to find a linear model that will reduce the mean square error.\n",
    "\n",
    "In the left image, we display the <font color=\"green\"> values of a model</font> for all values $x$ on a given interval.\n",
    "\n",
    "The right picture shows the value of *remainders* $y_i - h(\\vec{x}_i)$. The better the model fits the data, the less connected the dependent variable and the remainder will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "def plot_fit_residual(x, y, yp):\n",
    "\n",
    "    # Model\n",
    "    fig, axes  = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))\n",
    "    axes[0].plot(x.ravel(), y.ravel(), \"k.\",  label=\"Podatki\")\n",
    "    axes[0].plot(x.ravel(), yp.ravel(), \"g-\", label=\"Model h(x)\")\n",
    "    axes[0].set_xlabel(\"x\")\n",
    "    axes[0].set_ylabel(\"y\")\n",
    "    axes[0].legend(loc=4)\n",
    "\n",
    "    # Ostanki\n",
    "    r = pearsonr(y.ravel(), y.ravel()-yp.ravel())[0]\n",
    "    axes[1].plot(y.ravel(), y.ravel()-yp.ravel(), \"k.\", label=\"Ostanek\")\n",
    "    axes[1].set_xlabel(\"y\")\n",
    "    axes[1].set_ylabel(\"y-h(x)\")\n",
    "    axes[1].set_title(\"Graf ostankov, R=%.3f\" % r)\n",
    "    axes[1].legend(loc=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ucenje modela\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "print(model.intercept_,model.coef_)\n",
    "\n",
    "\n",
    "# Napoved vrednosti za podatke\n",
    "hx = model.predict(x)\n",
    "\n",
    "plot_fit_residual(x, y, hx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Izmerimo srednjo kvadratično napako..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Let's measure the mean square error ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_squared_error(hx, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "... ki je enaka varianci razlike. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "... which is equal to variance of difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(hx-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Tako lahko dobimo *delež pojasnene variance*. Delež v odstotkih si lažje intutivno razlagamo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "So we can get the *proportion of the explained variance*. The proportion in percent is easier to interpret intuitively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_var = 100.0 * ( np.var(y) - np.var(hx-y) ) /  np.var(y)\n",
    "print(\"Explained variance: %.2f \" % explained_var + \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slike/linreg_3.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "## Polinomska regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Oglejmo si naslednji motivacijski primer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Let's look at the next motivational example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"podatki/sintetični/data_B.txt\")\n",
    "x    = data[:, [0]]\n",
    "y    = data[:, [1]]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y, \"k.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Že na prvi pogled je jasno, da model premice ne bo zadostoval.  Če skozi podatke potegnemo premico, vidimo, da na nekaterih mesti pošteno zgreši podatke. To vidimo tudi na grafu ostankov, saj je napaka očitno odvisna od velikosti $y$, česar si ne želimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Already at first glance it is clear that the line model will not be enough. If we pull the line through the data, we see that in some places, the data is erroneous. This is also seen on the residual graph, since the error obviously depends on the size of $y$, which we do not want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "hx = model.predict(x)\n",
    "\n",
    "plot_fit_residual(x, y, hx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_var = 100.0 * ( np.var(y) - np.var(hx-y) ) /  np.var(y)\n",
    "print(\"Explained variance: %.2f \" % explained_var + \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "## Model polinomske regresije\n",
    "\n",
    "Z pomočjo linearnih modelov lahko modeliramo tudi nelinearne odvisnoti, kar je glede na začetne predpostavke nekoliko preseneneljivo. Vrednost $x$ je v tem primeru enodimenzionalna spremenljivka (p=1).\n",
    "\n",
    "**Model polinomske regresije** v eni dimenziji je polinom stopnje $D$:\n",
    "\n",
    "$$ h(\\vec{x}) = \\beta_1 x + \\beta_2 x^2 + ... + \\beta_D x^D + \\beta_0$$\n",
    "\n",
    "Učinek dosežemo z ustrezno priredbo prostora. Spremenljivko $x$ preslikamo v vektor tako, da izračunamo ustrezne potence:\n",
    "\n",
    "$$x \\rightarrow (x, x^2, x^3, ... x^D) = \\vec{x}$$\n",
    "\n",
    "V tako sestavljenem prostoru ni polinom nič drugega kot linearna preslikava! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Polynomial regression model\n",
    "\n",
    "Linear models can also model non-linear dependencies, which is somewhat surprising given the initial assumptions. The $x$ value in this case is a one-dimensional variable (p=1).\n",
    "\n",
    "**Polynomial regression model** in one dimension is a polynomial of degree $D$:\n",
    "\n",
    "$$ h(\\vec{x}) = \\beta_1 x + \\beta_2 x^2 + ... + \\beta_D x^D + \\beta_0$$\n",
    "\n",
    "The effect is achieved by appropriately arranging the space. The variable $x$ is mapped into a vector by calculating the corresponding potencies:\n",
    "\n",
    "$$x \\rightarrow (x, x^2, x^3, ... x^D) = \\vec{x}$$\n",
    "\n",
    "In such an assembly, there is nothing more than a linear mapping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iz 1-D sestavimo nov 2-D prostor\n",
    "X = np.zeros((len(x), 2))\n",
    "X[:, 0] = x.ravel()\n",
    "X[:, 1] = x.ravel()**2       \n",
    "\n",
    "# Učenje\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Napoved\n",
    "hx = model.predict(X)\n",
    "\n",
    "plot_fit_residual(x, y, hx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "##### Vprašanje 5-1-1\n",
    "Primerjaj pojasnjeno varianco linearnega in polinomskega modela."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "##### Question 5-1-1\n",
    "Compare the explained variance of the linear and polynomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "[Odgovor](rešitve_05-1_nadzorovano_linreg.ipynb#odgovor-5-1-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "[Answer](rešitve_05-1_nadzorovano_linreg.ipynb#odgovor-5-1-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slike/linreg_4.png\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "## Pretirano prileganje\n",
    "\n",
    "Optimalnega modela seveda pogosto ne poznamo. Uporaba pretirano kompleksnih modelov (kompleksnost si lahko predstavljamo kot velikost družine funkcij), lahko vodi v <b>pretirano prileganje</b> (ang. <i>overfitting</i>).\n",
    "\n",
    "Oglejmo si primer polinoma stopnje 20:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Overfitting\n",
    "\n",
    "Of course, we often do not know the optimal model. The use of excessively complex models (complexity can be represented as the size of a family of functions) can lead to **overfitting**.\n",
    "\n",
    "Let's look at the example of a 20 degree polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_coefficients(coef):\n",
    "    coef=coef.ravel()\n",
    "    D = len(coef)\n",
    "    plt.title(\"Parametri modela\")\n",
    "    plt.bar(np.arange(D), coef)\n",
    "    plt.xticks(np.arange(D))\n",
    "    plt.grid()\n",
    "    plt.ylabel(\"beta\")\n",
    "    plt.xlabel(\"d\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 20 # stopnja polinoma\n",
    "X = np.zeros((len(x), D))\n",
    "for d in range(0, D):\n",
    "    X[:, d] = x.ravel()**d\n",
    "    \n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "hx = model.predict(X)\n",
    "\n",
    "plot_fit_residual(X[:, 1], y, hx)\n",
    "plot_coefficients(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Model se na videz odlično prilega podatkom. Tudi graf ostankov kaže spodbudno sliko. Težava pretiranega prileganja se pojavi pri **napovedovanju novih podatkov**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "The model seems to fit the data perfectly. The graph of the remains also shows a stimulating picture. The problem of over-fitting occurs when **predicting new data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "##### Vprašanje 5-1-2\n",
    "Izmeri pojasnjeno varianco polinomskega modela."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "##### Question 5-1-2\n",
    "Measure the explained variance of the polynomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "[Odgovor](rešitve_05-1_nadzorovano_linreg.ipynb#Odgovor-5-1-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "[Answer](rešitve_05-1_nadzorovano_linreg.ipynb#Odgovor-5-1-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "## Rešitev: kaznovanje pretirano kompleksnih modelov "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Solution: punishing excessively complex models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "\n",
    "\n",
    "Poleg minimizacije srednje kvadratične napake lahko pri iskanju rešitve tudi *kaznujemo kompleksnost modelov*. Želimo torej, da so najdeni parametri v geometrijskem smislu čim manjši. Ta postopek je znan tudi kot regularizacija. Stopnjo regularizacije nadzoruje parameter $\\alpha$, ki ga določimo kot uporabniki. Dve najpogostejši različici modelov sta:\n",
    "* Regresija Lasso\n",
    "\n",
    "*\"Kaznovanje manhattanske razdalje vektorja $\\vec{\\beta}$ od izhodišča\"*\n",
    "\n",
    "$$ \\text{min}_{\\beta} \\sum_1^{n} (y_i - h(\\vec{x}_i))^2 + \\alpha \\|\\vec{\\beta}\\|_1 $$\n",
    "\n",
    "<font color=\"green\">Prednost</font>: vrača **redke** vektorje parametrov $\\vec{\\beta}$. Večina komponent $\\beta_j$ bo enaka 0 - <font color=\"green\">ZELO ZAŽELENO</font>!\n",
    "\n",
    "<font color=\"red\">Slabost</font>: zahtevno načrtovanje algoritmov za optimizacijo\n",
    "\n",
    "\n",
    "* Regresija Ridge\n",
    "*\"Kaznovanje evklidske razdalje vektorja $\\vec{\\beta}$ od izhodišča\"*\n",
    "$$ \\text{min}_{\\beta} \\sum_1^{n} (y_i - h(\\vec{x}_i))^2  + \\alpha \\|\\vec{\\beta}\\|_2 $$\n",
    "\n",
    "<font color=\"green\">Prednost</font>: Enostaven izračun\n",
    "\n",
    "<font color=\"red\">Slabost</font>: V splošnem ne vrača redkih vrednosti parametrov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "In addition to minimizing the mean square error, we can also penalize the *complexity of the models* when looking for a solution. Therefore, we want the parameters found in the geometric sense to be as small as possible. This procedure is also known as regularization. The degree of regularization is monitored by the parameter $\\alpha$, which is defined by the users. The two most common models are:\n",
    "* Regression Lasso\n",
    "\n",
    "*\"Punishment of the Manhattan distance of the vector $\\vec{\\beta}$ from the baseline\"*\n",
    "\n",
    "$$ \\text{min}_{\\beta} \\sum_1^{n} (y_i - h(\\vec{x}_i))^2 + \\alpha \\|\\vec{\\beta}\\|_1 $$\n",
    "\n",
    "<font color=\"green\">Pro</font>: returns **sparse** parameter vectors $\\vec{\\beta}$. Most of the components $\\beta_j$ will be 0 - <font color=\"green\">VERY DESIRABLE</font>!\n",
    "\n",
    "<font color=\"red\">Con</font>: complex planning of optimization algorithms\n",
    "\n",
    "\n",
    "* Regression Ridge\n",
    "*\"Penalizing the eclidic distance of the vector $\\vec{\\beta}$ from the starting point\"*\n",
    "$$ \\text{min}_{\\beta} \\sum_1^{n} (y_i - h(\\vec{x}_i))^2  + \\alpha \\|\\vec{\\beta}\\|_2 $$\n",
    "\n",
    "<font color=\"green\">Pro</font>: Easy calculation\n",
    "\n",
    "<font color=\"red\">Con</font>: Generally does not return rare parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 20 # stopnja polinoma\n",
    "\n",
    "# Ustvarimo ustrezen prostor\n",
    "X = np.zeros((len(x), D))\n",
    "for d in range(0, D):\n",
    "    X[:, d] = x.ravel()**d\n",
    "    \n",
    "model = Lasso(alpha=0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "hx = model.predict(X)\n",
    "\n",
    "plot_fit_residual(X[:, 1], y, hx)\n",
    "plot_coefficients(model.coef_)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "##### Vprašanje 5-1-3\n",
    "Kakšen je vpliv parametra ```alpha``` na a) kvaliteto prileganja b) koeficiente modela ? Poizkusi podatke modelirati z regresijo ```Ridge```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "##### Question 5-1-3\n",
    "What is the effect of the parameter ```alpha``` on a) the fitting quality, b) model coefficients? Try to model the data using regression ```Ridge```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "[Odgovor](rešitve_05-1_nadzorovano_linreg.ipynb#Odgovor-5-1-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "[Answer](rešitve_05-1_nadzorovano_linreg.ipynb#Odgovor-5-1-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Funkcija izgleda \"ravno pravi\" model za podatke. Na grafu koeficientov (parametrov) vidimo, da so večino teže dobili koeficienti nižjih stopenj polinoma, kar predstavlja manj kompleksen model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "The function looks like \"just the right\" model fpr the data. On the graph of the coefficients (parameters) we see that the coefficients of lower levels of the polynomial are most of the weight, which is a less complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "##### Vprašanje 5-1-4\n",
    "Poišči modele polinomske regresije za spodnje tri nabore podatkov. Izberi stopnjo polinoma ter morda vrsto regularizacijskega modela. Nariši graf funkcije in diagram ostankov. Komentiraj rezultate.\n",
    "\n",
    "Pravilne rešitve (koeficiente in stopnjo polinomov najdeš v ```podatki/sintetični/coefficients_*.txt```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "##### Question 5-1-4\n",
    "Find polynomial regression models for the following three sets of data. Choose the degree of the polynom and perhaps the type of the regularization model. Draw graph function and residue diagram. Comment the results.\n",
    "\n",
    "Correct solutions (you find the coefficients and degree of polynomials in ```podatki/sintetični/coefficients_*.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 6))\n",
    "\n",
    "for example, ax in zip([\"C\", \"D\", \"E\"], axes):\n",
    "    data = np.loadtxt(\"podatki/sintetični/data_%s.txt\" % example)\n",
    "    x    = data[:, [0]]\n",
    "    y    = data[:, [1]]\n",
    "    \n",
    "    ax.plot(x, y, \"k.\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_title(\"Primer %s\" % example)\n",
    "    \n",
    "    # ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "[Odgovor](rešitve_05-1_nadzorovano_linreg.ipynb#Odgovor-5-1-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "[Answer](rešitve_05-1_nadzorovano_linreg.ipynb#Odgovor-5-1-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slike/linreg_5.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "## Uporaba v praksi: analiza sentimenta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Use in practice: sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Za konec si oglejmo povsem praktičen primer uporabe regresijskih modelov. V podatkovni zbirki imamo 1101 recenzij knjig. Vsaka recenzija je sestavljena iz besedila (niz znakov, besed) in ocene med 1 in 5 (1-porazno, 5-odlično).\n",
    "Izvirna podatkovna zbirka in članek sta na voljo <a href=\"https://www.cs.jhu.edu/~mdredze/datasets/sentiment/\">tukaj</a>.\n",
    "\n",
    "Primer <font color=\"green\">pozitivne</font> recenzije ene izmed knjig (ocena = 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Finally, let's look at a completely practical example of using regression models. There are 1101 book reviews in the database. Each review consists of text (string of characters, words) and ratings between 1 and 5 (1-terrible, 5-excellent).\n",
    "The original database and article are available <a href=\"https://www.cs.jhu.edu/~mdredze/datasets/sentiment/\">here</a>.\n",
    "\n",
    "An example of a <font color=\"green\">positive</font> review of one of the books (rating = 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "I'm a little late in reading this book.  I am trying to pace myself between the movies and the books so that I can enjoy a little Harry Potter at least every year.  \n",
    "  I think Goblet of Fire is the best in the series, so naturally it would be pretty difficult for Phoenix to live up to that standard.  In Goblet, it feels like Rowling had a fantastic ending in mind, and that drove the writing of the story.  In Phoenix, it seems like the story is key and the ending was simply tacked on to round it out and provide some concluding information.  The major theme seemed to be the Ministry's takeover of Hogwarts.  I found that entire thread to be thoroughly engrossing.  Harry's dreams, however, took on sub-plot status, and then rose to the top for the final conflict.  \n",
    "  I didn't mind the length of the book, but it did seem to drag in a couple of places.  The gang spent too much time at Grimmauld Place.  I could have also done without Hagrid's giant story.  \n",
    "  My biggest problem with the book was Dumbledore's secrecy.  Good stories have real roadblocks to keep the hero from achieving his/her goal.  In Phoenix, the cause for most of the confusion was that Dumbledore was reluctant to share certain information with Harry.  He explains his behavior in a most unconvincing way in the final pages.  The secrecy was necessary for the story to take the course that it did, but it lacked good motivation and left me feeling let down.  It reminded me of the natural disaster movie template where the world could be easily saved if not for an arrogant bureaucrat who refuses to listen to the young upstart with all the answers.  It is contrived conflict that is hardly believable.\n",
    "  Don't get me wrong.  I love the Harry Potter series.  And, perhaps my expectations have risen too high.  I am eagerly awaiting the movie and then I know I will devour book 6 (when I allow myself to read it).  I lose interest in most book series after the first couple of installments.  Harry Potter always leaves me wanting more. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Primer <font color=\"red\">negativne</font>  recenzije ene izmed knjig (ocena = 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "An example of a <font color=\"red\">negative</font> review of one of the books (rating = 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "This book was horrible.  If it was possible to rate it lower than one star i would have.  I am an avid reader and picked this book up after my mom had gotten it from a friend.  I read half of it, suffering from a headache the entire time, and then got to the part about the relationship the 13 year old boy had with a 33 year old man and i lit this book on fire.  One less copy in the world...don't waste your money.\n",
    "\n",
    "I wish i had the time spent reading this book back so i could use it for better purposes.  This book wasted my life.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Vsako recenzijo predstavimo v prostoru 4000 najpogostejših besed oz. parov besed v podatkovni zbirki (predstavitev bag-of-words). Vsaka komponenta vrstice $x$ (vektorja) šteje, kolikorat se beseda/par besed pojavi v določeni recenziji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "We present each review in the space of the 4000 most common words or word pairs in the database (bag-of-words presentation). Each component of the line $x$ (vector) counts how many time a word/pair of words appears in a given review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from os.path import join\n",
    "\n",
    "def load_data(dset):\n",
    "    data = dict()\n",
    "\n",
    "    indir = \"podatki/%s/\" % dset\n",
    "\n",
    "    for name in \"data\", \"target\", \"data_test\", \"target_test\":\n",
    "        fname = join(indir, name + \".pkl\")\n",
    "        data[name] = load(open(fname, \"rb\"))\n",
    "\n",
    "        fname = join(indir, \"features.txt\")\n",
    "        fp = open(fname, \"rt\")\n",
    "        data[\"features\"] = list(map(lambda l: l.strip(), fp.readlines()))\n",
    "\n",
    "    return data\n",
    "\n",
    "books = load_data(\"books\")\n",
    "X = books[\"data\"]\n",
    "y = books[\"target\"]\n",
    "\n",
    "\n",
    "print(str(books['features'][:3]) + '...' + str(books['features'][-3:]))\n",
    "print(X.todense())\n",
    "print(y)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Vrstni red stolpcev v matriki $X$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "The order of columns in the matrix $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = books[\"features\"]\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Priložena je tudi podatkovna zbirka testnih primerov, kjer lahko testiramo napovedno točnost modela na novih podatkih."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "A database of test cases is also included, where we can test the predictive accuracy of the model on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = books[\"data_test\"]\n",
    "y_test = books[\"target_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "##### Vprašanje 5-1-5\n",
    "Uporabi omenjene linearne modele za modeliranje podatkov pri problemu analize sentimenta. Izmeri srednjo kvadratično napako in pojasnjeno varianco na testnih primerih."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "##### Question 5-1-5\n",
    "Use the aforementioned linear models for modeling data in the problem of the sentiment analysis. Measure the mean square error and the explained variance on test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "[Odgovor](rešitve_05-1_nadzorovano_linreg.ipynb#Odgovor-5-1-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "[Answer](rešitve_05-1_nadzorovano_linreg.ipynb#Odgovor-5-1-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "##### Vprašanje 5-1-6\n",
    "Ali lahko ugotoviš, katere besedne zveze močno pozitivno in močno negativno vplivajo na končno oceno recenzije? Namig: pomagaj si z vrednostjo koeficientov za posamezni stolpec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "##### Question 5-1-6\n",
    "Can you find out which phrases have a strong positive and strong negative impact on the final rating of the review? Tip: use the value of the coefficients for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "[Odgovor](rešitve_05-1_nadzorovano_linreg.ipynb#Odgovor-5-1-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "[Answer](rešitve_05-1_nadzorovano_linreg.ipynb#Odgovor-5-1-6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "sl",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
