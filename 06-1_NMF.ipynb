{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "# Nenegativna matrična faktorizacija in priporočilni sistemi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Non-negative matrix factorization and recommender systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Do sedaj smo obravnavali modele, ki so iz *več neodvisnih* napovedovali *eno* odvisno spremenljivko. V scenariju priporočilnega sistema smo tako za vsakega uporabnika zgradili svoj model.\n",
    "\n",
    "Glavna motivacija metod za priporočilne sisteme je, da modeli uporabnikov med sabo *niso neodvisni*. Želimo enoten model, ki bo ovrednotil poljubno kombinacijo uporabnika in izdelka, ter implicitno izkoriščal medsebojno informacijo med različnimi modeli uporabnikov. \n",
    "\n",
    "Eden od modelov, ki se zelo pogosto uporabljajo v praksi je model matrične faktorizacije.\n",
    "Ta predpostavlja matriko uporabnikov in izdelkov, ki ji predstavimo kot produkt dveh matrik *nižjega ranga*. Slednja lastnost omogoča stiskanje informacije in sklepanje o novih (ne-videnih, manjkajočih vrednosti) v izvirni matriki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "So far, we have considered models that predicted *one* dependent variable from *several independent*. In the scenario of a recommendation system, we have built a model for each user.\n",
    "\n",
    "The main motivation of methods for recommending systems is that user models are *not independent*. We want a single model that will evaluate any combination of user and product, and implicitly exploit mutual information between different user models.\n",
    "\n",
    "One of the models that are very commonly used in practice is the model of matrix factorization.\n",
    "This assumes a matrix of users and products that we present as a product of two matrices of a *lower rank*. The latter property allows compressing information and concluding new (unobserved, missing values) in the original matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "<img width=450 src=\"slike/nmf-shema.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "## Uvodne definicije\n",
    "\n",
    "Matriko podatkov $\\mathbf{X}$, ki vsebuje manjkajoče vrednosti, z modelom matrične faktorizacije predstavimo na naslednji način:\n",
    "$$ \\mathbf{X} = \\mathbf{W} \\mathbf{H}^T + \\mathbf{E} $$,\n",
    "\n",
    "torej kot produkt matrike $\\mathbf{W}$, ki predstavlja prostor vrstic, $\\mathbf{H}$ predstavlja prostor stolpcev, $\\mathbf{E}$ pa ostanek oz. napako. Matriki $\\mathbf{W}, \\mathbf{H}$ si včasih predstavljamo kot hkratno gručenje stolpcev in vrstic. Matrike so naslednjih velikosti:\n",
    "$$ \\mathbf{X} \\in \\mathbb{R}^{m \\times n}, \\mathbf{W} \\in \\mathbb{R}^{m \\times r}, \\mathbf{H} \\in \\mathbb{R}^{n \\times r},  \\mathbf{E} \\in \\mathbb{R}^{m \\times n}$$\n",
    "\n",
    "Predostavljamo, da sta matriki $\\mathbf{W}, \\mathbf{H}$ *nizkega ranga*, kar v praksi pomeni da celotno informacijo iz $\\mathbf{X}$ predstavljamo v stisnjeni obliki, torej\n",
    "$$r < m, r < n $$.\n",
    "\n",
    "Predpostavljamo tudi, da so matrike $\\mathbf{X}$, $\\mathbf{W}$ in $\\mathbf{H}$ nenegativne. Tedaj govorimo o **nenegativni matrični faktorizaciji (NMF)**.\n",
    "$$x_{i, j} > 0, w_{i, k} > 0, h_{j, k} > 0, \\forall i, j, k $$.\n",
    "\n",
    "Matrika napake $\\mathbf{E}$ te omejitve nima (<font color=\"blue\">razmisli</font>: zakaj?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Introductory definitions\n",
    "\n",
    "We can present the data matrix $\\mathbf{X}$ containing missing values with the matrix factorization model as follows:\n",
    "$$ \\mathbf{X} = \\mathbf{W} \\mathbf{H}^T + \\mathbf{E} $$,\n",
    "\n",
    "therefore as a product of the $\\mathbf{W}$ matrix representing the row space, $\\mathbf{H}$ represents the column space, and $\\mathbf{E}$ is the residue or error. The $\\mathbf{W}, \\mathbf{H}$ matrices are sometimes represented as a concurrent clustering of columns and rows. Matrices are of the following sizes:\n",
    "$$ \\mathbf{X} \\in \\mathbb{R}^{m \\times n}, \\mathbf{W} \\in \\mathbb{R}^{m \\times r}, \\mathbf{H} \\in \\mathbb{R}^{n \\times r}, \\mathbf{E} \\in \\mathbb{R}^{m \\times n}$$\n",
    "\n",
    "We assume that the matrices $\\mathbf{W}, \\mathbf{H}$ are of *low rank*, which in practice means that the entire information from $\\mathbf{X}$ is presented in a compressed form, that is,\n",
    "$$r < m, r < n $$.\n",
    "\n",
    "We also assume that the matrices $\\mathbf{X}$, $\\mathbf{W}$ and $\\mathbf{H}$ are non-negative. Then we talk about **non-negative matrix factorization (NMF)**.\n",
    "$$x_{i, j} > 0, w_{i, k} > 0, h_{j, k} > 0, \\forall i, j, k $$.\n",
    "\n",
    "The $\\mathbf{E}$ error matrix does not have this limit (<font color=\"blue\">think</font>: why?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "## Definicija problema\n",
    "\n",
    "Želimo torej poiskati matriki $\\mathbf{W}$ in $\\mathbf{H}$, tako da vrednost napake karseda nizka. To lahko zapišemo kot naslednji optimizacijski problem:\n",
    "\n",
    "$$ \\text{min}_{\\mathbf{W},\\mathbf{H}}\\ \\| \\mathbf{X} - \\mathbf{W}\\mathbf{H}^T \\|_F^2 = \\text{min}_{\\mathbf{W},\\mathbf{H}}\\ J$$\n",
    "\n",
    "Oznaka $\\| \\mathbf{A} \\|_F = \\sqrt{\\sum_{i,j} a_{i,j}^2}$ predstavlja *Frobeniusovo normo* matrike $\\mathbf{A}$.  (<font color=\"blue\">razmisli</font>: Opaziš podobnost s srednjo kvadratično napako, ki smo jo spoznali v kontekstu linearne regresije?)\n",
    "\n",
    "Vrednost $J$ imenujemo *kriterijska funkcija*, problem iskanja minimuma pa *optimizacijski oz. minimizacijski problem*.  **Posebnost** priporičilnih sistemov je ta, da napako računamo samo na vrednostih v $\\mathbf{X}$, ki so znane. Kriterijska funkcija je torej:\n",
    "\n",
    "$$ J = \\sum_{i, j | x_{i,j} \\not = 0} (x_{i, j} - \\sum_{l=1}^{r} w_{i,l}h_{j, l} )^2 $$\n",
    "\n",
    "Za ta konkreten problem velja, da nima globalno optimalne rešitve za spremenljivke $\\mathbf{W},\\mathbf{H}$.  Vseeno ga lahko rešimo npr. z odvajanjem kriterijske funkcije in premikanjem v negativni smeri gradienta. Dobimo \n",
    "*pravila za posodabljanje* vrednosti v $\\mathbf{W},\\mathbf{H}$:\n",
    "\n",
    "Vse vrednosti $w_{i,k}$ in $h_{j, k}$ popravimo tako, da vrednost v prejšnji iteraciji *popravimo* v negativni smeri gradienta, s *korakom* $\\eta$:\n",
    "\n",
    "$$ w_{i,k}^{(t+1)}  = w_{i, k}^{(t)} - \\eta \\frac{\\delta J}{\\delta w_{i,k}} = w_{i, k}^{(t)} + \\eta \\sum_{j \\ | \\ x_{i,j} \\not = 0} (x_{i,j} - \\sum_{l=1}^r w_{i,l} h_{j, l})(w_{i, k}^{(t)})$$\n",
    "\n",
    "$$ h_{j, k}^{(t+1)}  = h_{j, k}^{(t)} - \\eta \\frac{\\delta J}{\\delta h_{j, k}} = h_{j, k}^{(t)} + \\eta \\sum_{i \\ | \\  x_{i,j} \\not = 0} (x_{i,j} - \\sum_{l=1}^r w_{i,l} h_{j, l})(h_{j, k}^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Problem definition \n",
    "\n",
    "We want to find the matrices $\\mathbf{W}$ and $\\mathbf{H}$ so that the error value is as low as possible. This can be written as the following optimization problem:\n",
    "\n",
    "$$ \\text{min}_{\\mathbf{W},\\mathbf{H}}\\ \\| \\mathbf{X} - \\mathbf{W}\\mathbf{H}^T \\|_F^2 = \\text{min}_{\\mathbf{W},\\mathbf{H}}\\ J$$\n",
    "\n",
    "The $\\| \\mathbf{A} \\|_F = \\sqrt{\\sum_{i,j} a_{i,j}^2}$ notation represents the *Frobenius norm* of matrix $\\mathbf{A}$. (<font color=\"blue\">think</font>: Do you see the similarity with the mean square error that we have seen in the context of linear regression?)\n",
    "\n",
    "The value $J$ is called *criterion function*, and the problem of searching for a minimum is *optimization or minimization problem*. **The particularity** of recommender systems is that we calculate the error only on the known values in $\\mathbf{X}$. The criterion function is therefore:\n",
    "\n",
    "$$ J = \\sum_{i, j | x_{i,j} \\not = 0} (x_{i, j} - \\sum_{l=1}^{r} w_{i,l}h_{j, l} )^2 $$\n",
    "\n",
    "This particular problem does not have a globally optimal solution for the variables $\\mathbf{W},\\mathbf{H}$. However, it can be solved, for example, by deriving the criterion function and moving in the negative direction of the gradient. We get \n",
    "*update rules* for values in $\\mathbf{W},\\mathbf{H}$:\n",
    "\n",
    "All values of $w_{i,k}$ and $h_{j, k}$ are corrected so that the value in the previous iteration *is corrected* in the negative direction of the gradient, with *step* $\\eta$:\n",
    "\n",
    "$$ w_{i,k}^{(t+1)}  = w_{i, k}^{(t)} - \\eta \\frac{\\delta J}{\\delta w_{i,k}} = w_{i, k}^{(t)} + \\eta \\sum_{j \\ | \\ x_{i,j} \\not = 0} (x_{i,j} - \\sum_{l=1}^r w_{i,l} h_{j, l})(w_{i, k}^{(t)})$$\n",
    "\n",
    "$$ h_{j, k}^{(t+1)}  = h_{j, k}^{(t)} - \\eta \\frac{\\delta J}{\\delta h_{j, k}} = h_{j, k}^{(t)} + \\eta \\sum_{i \\ | \\  x_{i,j} \\not = 0} (x_{i,j} - \\sum_{l=1}^r w_{i,l} h_{j, l})(h_{j, k}^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "## Stohastični gradientni sestop\n",
    "\n",
    "Stohastični gradientni sestop (SGD) je postopek za reševanje optimizcijskih problemov, ki niso globalno rešljivi, za vse nastopajoče spremenljivke (v našem primeru vse $w_{i,k}$ in $h_{j, k}$) pa znamo izračunati odvod glede na kriterijsko funkcijo. To smo storili v prešnjem delu.\n",
    "Postopek za iskanje *lokalnega minimuma* je naslednji.\n",
    "\n",
    "1. Naključno nastavi vrednosti vseh spremenljivk  $w_{i,k}$ in $h_{j, k}$. V našem primeru \n",
    "    velja $w_{i,k} > 0$  in $h_{j, k} > 0$.\n",
    "2. V iteraciji $t = 1...T$:\n",
    "    \n",
    "    2.1 V naključnem vrstnem redu posodabljaj $\\forall i, k, j$\n",
    "$$ w_{i,k}^{(t+1)}  = w_{i, k}^{(t)} - \\eta \\frac{\\delta J}{\\delta w_{i,k}} $$\n",
    "$$ h_{j, k}^{(t+1)}  = h_{j, k}^{(t)} - \\eta \\frac{\\delta J}{\\delta h_{j, k}} $$\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Stochastic gradient descent\n",
    "\n",
    "Stochastic gradient descent (SGD) is a procedure for solving optimization problems that are not globally solvable, so we can calculate the derivative according to the criterion function for all the variables (in our case all $w_{i,k}$ and $h_{j, k}$). We did this in the previous part.\n",
    "The procedure for searching the *local minimum* is as follows.\n",
    "\n",
    "1. Randomly set the values of all the variables $w_{i,k}$ and $h_{j, k}$. In our case\n",
    "    $w_{i,k} > 0$ and $h_{j, k} > 0$ apply.\n",
    "2. In the iteration $t = 1...T$:\n",
    "    \n",
    "    2.1 In the random order, update $\\forall i, k, j$\n",
    "$$ w_{i,k}^{(t+1)}  = w_{i, k}^{(t)} - \\eta \\frac{\\delta J}{\\delta w_{i,k}} $$\n",
    "$$ h_{j, k}^{(t+1)}  = h_{j, k}^{(t)} - \\eta \\frac{\\delta J}{\\delta h_{j, k}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=450 src=\"slike/gradient-descent-1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Shematski prikaz gradientnega sestopa za hipotetični spremenljivki $w$, $h$ in kriterijsko funkcijo $J(w, h)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Schematic representation of the gradient descent for the hypothetical variable $w$, $h$ and the criterion function $J(w, h)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "##### Vprašanje 6-1-1\n",
    "Dopolni spodnjo implementacijo algoritma NMF, tako da uporabiš posodobitvena pravila v več iteracijah stohastičnega gradientnega sestopa. \n",
    "<br/>\n",
    "<font color=\"blue\">**Namig.**</font> Pri računanju gradienta upoštevaj samo vrednosti $x_{i, j}$, ki so znane (različne od 0). Za učinkovito implementacijo izračuna vsot\n",
    "$\\sum_{i \\ | \\  x_{i,j} \\not = 0}$ in $\\sum_{j \\ | \\  x_{i,j} \\not = 0}$ najprej (pred začetkom iteracij):\n",
    "* za vsako vrstico $i$ shranimo neničelne stolpce\n",
    "* za vsak stolpec $j$ shranimo neničelne vrstice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "##### Question 6-1-1\n",
    "Complete the implementation of the NMF algorithm below by using the update rules in several iterations of a stochastic gradient descent.\n",
    "<br/>\n",
    "<font color=\"blue\"> **Hint.** </font> Only calculate $x_{i, j}$ values that are known (different from 0) when calculating the gradient. For effective implementation, calculate sums\n",
    "$\\sum_{i \\ | \\  x_{i,j} \\not = 0}$ and $\\sum_{j \\ | \\  x_{i,j} \\not = 0}$ first (before the beginning of iterations):\n",
    "* for each row of $i$ we store non-columnar columns\n",
    "* for each column $j$ we store non-linear rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "class NMF:\n",
    "    \n",
    "    \"\"\"\n",
    "    Fit a matrix factorization model for a matrix X with missing values.\n",
    "    such that\n",
    "        X = W H.T + E \n",
    "    where\n",
    "        X is of shape (m, n)    - data matrix\n",
    "        W is of shape (m, rank) - approximated row space\n",
    "        H is of shape (n, rank) - approximated column space\n",
    "        E is of shape (m, n)    - residual (error) matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rank=10, max_iter=100, eta=0.01):\n",
    "        \"\"\"\n",
    "        :param rank: Rank of the matrices of the model.\n",
    "        :param max_iter: Maximum nuber of SGD iterations.\n",
    "        :param eta: SGD learning rate.\n",
    "        \"\"\"\n",
    "        self.rank = rank\n",
    "        self.max_iter = max_iter\n",
    "        self.eta = eta\n",
    "    \n",
    "    \n",
    "    def fit(self, X, verbose=False):\n",
    "        \"\"\"\n",
    "        Fit model parameters W, H.\n",
    "        :param X: \n",
    "            Non-negative data matrix of shape (m, n)\n",
    "            Unknown values are assumed to take the value of zero (0).\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        \n",
    "        W = np.random.rand(m, self.rank)\n",
    "        H = np.random.rand(n, self.rank)\n",
    " \n",
    "        # Indices to model variables\n",
    "        w_vars = list(itertools.product(range(m), range(self.rank)))\n",
    "        h_vars = list(itertools.product(range(n), range(self.rank)))\n",
    "\n",
    "        # Indices to nonzero rows/columns\n",
    "        nzcols = dict([(j, X[:, j].nonzero()[0]) for j in range(n)])\n",
    "        nzrows = dict([(i, X[i, :].nonzero()[0]) for i in range(m)])\n",
    "\n",
    "        # nzrows[i] <- vrni stolpce j, tako da x_ij > 0\n",
    "        \n",
    "        \n",
    "        # Errors\n",
    "        self.error = np.zeros((self.max_iter,))\n",
    "\n",
    "        for t in range(self.max_iter):\n",
    "            np.random.shuffle(w_vars)\n",
    "            np.random.shuffle(h_vars)\n",
    "\n",
    "            for i, k in w_vars:\n",
    "                # TODO: your code here\n",
    "                # Calculate gradient and update W[i, k]\n",
    "                pass\n",
    "\n",
    "            for j, k in h_vars:\n",
    "                # TODO: your code here\n",
    "                # Calculate gradient and update H[j, k]\n",
    "                pass\n",
    " \n",
    "            self.error[t] = np.linalg.norm((X - W.dot(H.T))[X > 0])**2\n",
    "            if verbose: print(t, self.error[t])\n",
    "        \n",
    "        self.W = W\n",
    "        self.H = H\n",
    "    \n",
    "    \n",
    "    def predict(self, i, j):\n",
    "        \"\"\"\n",
    "        Predict score for row i and column j\n",
    "        :param i: Row index.\n",
    "        :param j: Column index.\n",
    "        \"\"\"\n",
    "        return self.W[i, :].dot(self.H[j, :])\n",
    "    \n",
    "\n",
    "    def predict_all(self):\n",
    "        \"\"\"\n",
    "        Return approximated matrix for all\n",
    "        columns and rows.\n",
    "        \"\"\"\n",
    "        return self.W.dot(self.H.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rešitev najdete v rešitve/nmf.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run rešitve_06-1_NMF.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Testirajmo metodo na matriki naključnih podatkov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Test the method on a matrix of random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 100       # St. vrstic\n",
    "n = 80        # St. stolpcev\n",
    "rank = 5      # Rang model\n",
    "error = 0.1   # Nakljucni šum\n",
    "A = np.random.rand(m, rank*2)  \n",
    "B = np.random.rand(n, rank*2)\n",
    "X = A.dot(B.T) + error * np.random.rand(m, n)  # generiramo podatke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "lang": "sl"
   },
   "source": [
    "Poženemo iskanje parametrov $\\mathbf{W}$, $\\mathbf{H}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "We start searching the parameters $\\mathbf{W}$, $\\mathbf{H}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NMF(rank=rank, max_iter=20, eta=0.001)\n",
    "model.fit(X, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Napaka modela pada s številom iteracij."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Error of the model falls with number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'jpg'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(model.error)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Objective function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Primerjajmo model in izvirne podatke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Let's compare the model and the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "ax[0].pcolor(X)\n",
    "ax[0].set_title(\"Original\")\n",
    "\n",
    "ax[1].pcolor(model.predict_all())\n",
    "ax[1].set_title(\"Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "Izračunamo pojasnjeno varianco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "We calculate the explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xp = model.predict_all()\n",
    "expl_var = (np.var(X) - np.var(X-Xp))/np.var(X)\n",
    "expl_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "##### Vprašanje 6-1-2\n",
    "Kako se pojasnjena varianca spreminja z rangom modela, št. iteracij?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "##### Question 6-1-2\n",
    "How does the explained variance change with the rang of the model, no. of iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for rank in range(3, 10):\n",
    "    model = NMF(rank=rank, max_iter=20, eta=0.001)\n",
    "    model.fit(X)\n",
    "    Xp = model.predict_all()\n",
    "    expl_var = (np.var(X) - np.var(X-Xp))/np.var(X)\n",
    "    print(rank, expl_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "##### Vprašanje 6-1-3\n",
    "Preizkusi metodo NMF na podatkovni zbirki Jester. Podatki so razdeljeni na učno in testno množico, kjer je v učni množici prisoten delež $p$ ocen. Poženi model na učni množici in izračunaj testno napako (RMSE, pojasnjeno varianco) na ocenah, ki niso bile uporabljene za učenje. Izračunaj, kako se testna napaka spreminja v odvisnosti od:\n",
    "* delež učnih ocen $p$,\n",
    "* ranga matrik modela (število $r$, parameter ```rank```)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "##### Question 6-1-3\n",
    "Test the NMF method on the Jester database. The data are divided into a learning and test set, where a share $p$ of ratings is present in the learning set. Run the model on the learning set and calculate the test error (RMSE, explained variance) on estimates that were not used for learning. Calculate how the test error varies depending on:\n",
    "* the share of learning estimates of $p$,\n",
    "* rank matrix of the model (number $r$, parameter ```rank```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naložimo podatkovno zbirko Jester z 1% upoštevanih ocen\n",
    "def load_jester(p=0.05):\n",
    "    \"\"\"\n",
    "    :param p: Probability of rating appearing in the training set.\n",
    "    :return\n",
    "        X training grades (retining with probability p)\n",
    "        Y test grades (whole dataset) \n",
    "    \"\"\"\n",
    "\n",
    "    Y = np.genfromtxt(\"podatki/jester-data.csv\", delimiter=\",\", dtype=float, )\n",
    "    Y = Y[:, 1:]\n",
    "    Y[Y == 99] = 0 \n",
    "    Y[Y != 0]  = Y[Y!=0] + abs(Y[Y!=0].min())\n",
    "\n",
    "    # Separate data in test/train with probability p\n",
    "    M = np.random.rand(*Y.shape) \n",
    "    M_tr = M < p\n",
    "    M_te = M > p\n",
    "    X = Y * M_tr\n",
    "    Y = Y * M_te\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "# X: 1% podatkov, Y ostalih 99% \n",
    "X, Y = load_jester(p=0.5)\n",
    "\n",
    "X = X[:1000, :]\n",
    "Y = Y[:1000, :]\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)\n",
    "\n",
    "print(\"X, Nonzeros:\", np.sum(X>0), \"Total:\", X.shape[0]*X.shape[1])\n",
    "print(\"Y, Nonzeros:\", np.sum(Y>0), \"Total:\", Y.shape[0]*Y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NMF(rank=10, max_iter=100, eta=0.001)\n",
    "model.fit(X)\n",
    "Yp = model.predict_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(model.error)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"J value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "ax[0].pcolor(Y)\n",
    "ax[0].set_title(\"Original\")\n",
    "\n",
    "ax[1].pcolor(Yp)\n",
    "ax[1].set_title(\"Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expl_var = (np.var(Y[Y>0]) - np.var(Y[Y>0] - Yp[Y>0])) / np.var(Y[Y>0])\n",
    "print(expl_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "##### Vprašanje 6-1-4\n",
    "Na podatkovni zbirki Jester izberite eno celico z vrednostjo različno od 0 in jo nastavite na 0. Matriko faktorizirajte in napovejte vrednost te celice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "##### Question 6-1-4\n",
    "On the Jester database, select one cell with a value other than 0, and set it to 0. Factorize and predict the value of this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "##### Vprašanje 6-1-5\n",
    "Poiščite celice, kjer je razlika med aproksimirano in originalno matriko največja. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "##### Question 6-1-5\n",
    "Locate cells where the difference between the approximate and the original matrix is ​​greatest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "sl"
   },
   "source": [
    "##### Vprašanje 6-1-6\n",
    "Ustvarite priporočilni sistem. Izberite nekaj uporabnikov in za vsakega izpišite pet še neocenjenih šal, ki mu bodo najbolj všeč, glede na napoved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "##### Question 6-1-6\n",
    "Create a recommended system. Choose a few users and for each of them output five yet unrated jokes that they will like the most, according to the prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "sl",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
